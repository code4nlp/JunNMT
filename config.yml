# Network
encoder_type:
decoder_type: AttnDecoderRNN
rnn_type: GRU #RNN, LSTM, GRU, SRU
bidirectional: false
embedding_size: 100
hidden_size: 100
num_layers: 1
dropout: 0.3
atten_model: general #general, dot, none
param_init: 0.1

src_max_len: 50
tgt_max_len: 50

# Vocab
share_embedding: false

# Set vocab limit size
src_vocab_size: 50000
tgt_vocab_size: 50000


# File
train_src_file: /home/gaojun4ever/Documents/Projects/mt-exp/data/train/train.cn
train_tgt_file: /home/gaojun4ever/Documents/Projects/mt-exp/data/train/train.en


dev_src_file: /home/gaojun4ever/Documents/Projects/mt-exp/data/dev/nist02.cn
dev_tgt_file: 
  - /home/gaojun4ever/Documents/Projects/mt-exp/data/dev/nist02.en0
  - /home/gaojun4ever/Documents/Projects/mt-exp/data/dev/nist02.en1
  - /home/gaojun4ever/Documents/Projects/mt-exp/data/dev/nist02.en2
  - /home/gaojun4ever/Documents/Projects/mt-exp/data/dev/nist02.en3

src_vocab_file: /home/gaojun4ever/Documents/Projects/mt-exp/data/src_vocab.cn
tgt_vocab_file: /home/gaojun4ever/Documents/Projects/mt-exp/data/src_vocab.en

# Misc
USE_CUDA: true

# Train
optim_method: adam #adadelta, adam, sgd
max_grad_norm: 5
learning_rate: 0.001
learning_rate_decay: 0.5
weight_decay: 0 #  weight decay(L2 penalty)
num_train_epochs: 100
steps_per_stats: 100
steps_per_eval: 1000
batch_size: 20
out_dir: ./out_dir
checkpoint: 


# Translate
replace_unk: true
decode_max_length: 100
beam_size: 3

# Tensorboard
log_dir: ./log_dir